{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extend the ideas developed in the previous notebook to build a neural network using functions;\n",
    "- Implement functions for the initialization of, forward propagation through, and updating of a neural network;\n",
    "- Apply the logic of backpropagation in more detail;\n",
    "- Hand-code gradient descent using `numpy` for a more realistic size of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent spent some time working on the ideas of supervised learning and getting familiar with the terminology of neural networks, let's write some code to implement a neural network from scratch. We're going to use a functional programming style to help build intuition. To make matters easier, we'll use a dictionary called `model` to store all data associated with the neural network (the weight matrices, the  bias vectors, etc.) and pass that into functions as a single argument. We'll also assume that the activation functions are the same in all the layers (i.e., the *logistic* or *sigmoid* function) to simplify the implementation. Production codes usually use an object-oriented style to build networks and, of course, are optimized for efficiency (unlike what we'll develop here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to borrow notation from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com) to make life easier. In particular, we'll let $W^\\ell$ and $b^\\ell$ denote the weight matrices & bias vectors respectively associated with the $\\ell$th layer of the network. The entry $W^{\\ell}_{jk}$ of $W^\\ell$ is the weight parameter associated with the link connecting the $k$th neuron in layer $\\ell-1$ to the $j$th neuron in layer $\\ell$:\n",
    "\n",
    "[![](../img/tikz16.png)](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Let's put this altogether now and construct a network from scratch. We start with some typical imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an initialization function to set up model\n",
    "\n",
    "Rather than the fixed constants in `setup` from before, write a function `initialize_model` that accepts a list  `dimensions` of positive integer inputs that constructs a `dict` with specific key-value pairs:\n",
    "+ `model['nlayers']` : number of layers in neural network\n",
    "+ `model['weights']` : list of NumPy matrices with appropriate dimensions\n",
    "+ `model['biases']` : list of NumPy (column) vectors of appropriate dimensions\n",
    "+ The matrices in `model['weights']` and the vectors in `model['biases']` should be initialized as randomly arrays of the appropriate shapes.\n",
    "\n",
    "If the input list `dimensions` has `L+1` entries, the number of layers is `L` (the first entry of `dimensions` is the input dimension, the next ones are the number of units/neurons in each subsequent layer going up to the output layer).\n",
    "Thus, for example:\n",
    "\n",
    "```python\n",
    ">>> dimensions = [784, 15, 10]\n",
    ">>> model = initialize_model(dimensions)\n",
    ">>> for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    ">>>    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')\n",
    "```\n",
    "```\n",
    "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
    "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(dimensions):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers'] : number of layers in neural network\n",
    "      model['weights'] : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases'] : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    weights, biases = [], []\n",
    "    L = len(dimensions) - 1 # number of layers (i.e., excludes input layer)\n",
    "    for l in range(L):\n",
    "        W = np.random.randn(dimensions[l+1], dimensions[l])\n",
    "        b = np.random.randn(dimensions[l+1], 1)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return dict(weights=weights, biases=biases, nlayers=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['weights', 'biases', 'nlayers'])\n"
     ]
    }
   ],
   "source": [
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "print(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement activation function(s), loss functions, & their derivatives\n",
    "For today's purposes, we'll use only the *logistic* or *sigmoid* function as an activation function:\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{1+\\exp(x)}.$$\n",
    "A bit of calculus shows that\n",
    "$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) .$$\n",
    "\n",
    "Actually, a more numerically robust formula for $\\sigma(x)$ (i.e., one that works for large positive or large negative input equally well) is\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases} \\frac{1}{1+\\exp(-x)} & (x\\ge0) \\\\ 1 - \\frac{1}{1+\\exp(x)} & \\mathrm{otherwise} \\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, we'll use the typical \"$L_2$-norm of the error\" (alternatively called *mean-square error (MSE)* when averaged over a batch of values:\n",
    "$$ \\mathcal{E}(\\hat{y},y) = \\frac{1}{2} \\|\\hat{y}-y\\|^{2} = \\frac{1}{2} \\sum_{k=1}^{d} \\left[ \\hat{y}_{k}-y_{k} \\right]^{2}.$$\n",
    "Again, using multivariable calculus, we can see that\n",
    "$$\\nabla_{\\hat{y}} \\mathcal{E}(\\hat{y},y) = \\hat{y} - y.$$\n",
    "\n",
    "Implement all four of these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return np.where(x>=0, 1/(1+np.exp(-x)), 1 - 1/(1+np.exp(x))) # piecewise for numerical robustness\n",
    "def sigma_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return sigma(x)*(1-sigma(x)) # Derivative of logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    return 0.5 * np.square(yhat-y).sum()\n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    return (yhat - y) # gradient w.r.t. yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement a function for forward propagation\n",
    "\n",
    "Write a function `forward` that uses the architecture described in a `dict` as created by `initialize_model` to evaluate the output of the neural network for a given input *column* vector `x`.\n",
    "+ Take $a^{0}=x$ from the input.\n",
    "+ For $\\ell=1,\\dotsc,L$, compute & store the intermediate computed vectors $z^{\\ell}=W^{\\ell}a^{\\ell-1}+b^{\\ell}$ (the *weighted inputs*) and $a^{\\ell}=\\sigma\\left(z^{\\ell}\\right)$ (the *activations*) in an updated dictionary `model`. That is, modify the input dictionary `model` so as to accumulate:\n",
    "  + `model['activations']`: a list with entries $a^{\\ell}$ for $\\ell=0,\\dotsc,L$\n",
    "  + `model['z_inputs']`: a list with entries $z^{\\ell}$ for $\\ell=1,\\dotsc,L$\n",
    "+ The function should return the computed output $a^{L}$ and the modified dictionary `model`.\n",
    "Notice that input `z` can be a matrix of dimension $n_{0} \\times N_{\\mathrm{batch}}$ corresponding to a batch of input vectors (here, $n_0$ is the dimension of the expected input vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract process into function and run tests again.\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation through a feed-forward neural network.\n",
    "       x : input array oriented column-wise (i.e., features along the rows)\n",
    "       model : dict with same keys as output of initialize_model & appropriate lists in 'weights' & 'biases'\n",
    "    The output dict model is the same as the input with additional keys 'z_inputs' & 'activations';\n",
    "    these are accumulated to be used later for backpropagation. Notice the lists model['z_inputs'] &\n",
    "    model['activations'] both have the same number of entries as model['weights'] & model['biases']\n",
    "    (one for each layer).\n",
    "    '''\n",
    "    a = x\n",
    "    activations = [a]\n",
    "    zs = []\n",
    "    for W, b in zip(model['weights'], model['biases']):\n",
    "        z = W @ a + b\n",
    "        a = sigma(z)\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    model['activations'], model['z_inputs'] = activations, zs\n",
    "    return (a, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784,15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for backpropagation:\n",
    "\n",
    "#### (optional reading for the mathematically brave)\n",
    "\n",
    "The description here is based on the *wonderfully concise* description from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/chap2.html). Neilsen has artfully crafted a summary using the bare minimum mathematical prerequisites. The notation elegantly summarises the important ideas in a way to make implementation easy in array-based frameworks like Matlab or NumPy. This is the best description I (Dhavide) know of that does this.\n",
    "\n",
    "In the following, $\\mathcal{E}$ is the loss function and the symbol $\\odot$ is the [*Hadamard product*](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of two conforming arrays; this is simply a fancy way of writing the usual element-wise product of arrays as computed by NumPy & is sometimes called the *Schur product*. This can be reformulated in usual matrix algebra for analysis.\n",
    "\n",
    "Given a neural network with $L$ layers (not including the \"input layer\") described by an appropriate architecture:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a^{0} \\leftarrow x$ for the input layer.\n",
    "2. Feedforward: For each $\\ell=1,2,\\dotsc,L$, compute *weighted inputs* $z^{\\ell}$ & *activations* $a^{\\ell}$ using the formulas\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{\\ell} & \\leftarrow  W^{\\ell} a^{\\ell-1} + b^{\\ell}, \\\\\n",
    "a^{\\ell} & \\leftarrow  \\sigma\\left( z^{\\ell}\\right)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "3. Starting from the end, compute the \"error\" in the output layer $\\delta^{L}$ according to the formula\n",
    "$$\n",
    "\\delta^{L} \\leftarrow \\nabla_{a^{L}} \\mathcal{E} \\odot \\sigma'\\left(z^{L}\\right)\n",
    "$$\n",
    "\n",
    "4. *Backpropagate* the \"error\" for $\\ell=L−1\\dotsc,1$ using the formula\n",
    "$$\n",
    "\\delta^{\\ell} \\leftarrow \\left[ W^{\\ell+1}\\right]^{T}\\delta^{\\ell+1} \\odot \\sigma'\\left(z^{\\ell}\\right).\n",
    "$$\n",
    "5. The required gradients of the loss function $\\mathcal{E}$ with respect to the parameters $W^{\\ell}_{p,q}$ and $b^{\\ell}_{r}$ can be computed directly from the \"errors\" $\\left\\{ \\delta^{\\ell} \\right\\}$ and the weighted inputs $\\left\\{ z^{\\ell} \\right\\}$ according to the relations\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}} &= a^{\\ell-1}_{q} \\delta^{\\ell}_{p} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &= \\delta^{\\ell}_{r} &&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement a function for backward propagation\n",
    "\n",
    "**This one is a freebie!**\n",
    "\n",
    "Implement a function `backward` that implements the back-propagation algorithm to compute the gradients of the loss function $\\mathcal{E}$ with respect to the weight matrices $W^{\\ell}$ and the bias vectors $b^{\\ell}$.\n",
    "+ The function should accept a column vector `y` of output labels and an appropriate dictionary `model` as input.\n",
    "+ The dict `model` is assumed to have been generated *after* a call to `forward`; that is, `model` should have keys `'w_inputs'` and `'activations'` as computed by a call to `forward`.\n",
    "+ The result will be a modified dictionary `model` with two additional key-value pairs:\n",
    "  + `model['grad_weights']`: a list with entries $\\nabla_{W^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "  + `model['grad_biases']`: a list with entries $\\nabla_{b^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "+ Notice the dimensions of the matrices $\\nabla_{W^{\\ell}} \\mathcal{E}$ and the vectors $\\nabla_{b^{\\ell}} \\mathcal{E}$ will be identical to those of ${W^{\\ell}}$ and ${b^{\\ell}}$ respectively.\n",
    "+ The function's return value is the modified dictionary `model`.\n",
    "\n",
    "\n",
    "We've done this for you (in the interest of time). Notice that input `y` can be a matrix of dimension $n_{L} \\times N_{\\mathrm{batch}}$ corresponding to a batch of output vectors (here, $n_L$ is the number of units in the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network\n",
    "       y : output array oriented column-wise (i.e., features along the rows) as output by forward\n",
    "       model : dict with same keys as output by forward\n",
    "    Note the input needs to have keys 'nlayers', 'weights', 'biases', 'z_inputs', and 'activations'\n",
    "    '''\n",
    "    Nbatch = y.shape[1] # Needed to extend for batches of vectors\n",
    "    # Compute the \"error\" delta^L for the output layer\n",
    "    yhat = model['activations'][-1]\n",
    "    z, a = model['z_inputs'][-1], model['activations'][-2]\n",
    "    delta = loss_prime(yhat, y) * sigma_prime(z)\n",
    "    # Use delta^L to compute gradients w.r.t b & W in the output layer.\n",
    "    grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "    grad_weights, grad_biases = [grad_W], [grad_b]\n",
    "    loop_iterates = zip(model['weights'][-1:0:-1],\n",
    "                        model['z_inputs'][-2::-1],\n",
    "                        model['activations'][-3::-1])\n",
    "    for W, z, a in loop_iterates:\n",
    "        delta = np.dot(W.T, delta) * sigma_prime(z)\n",
    "        grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "        grad_weights.append(grad_W)\n",
    "        grad_biases.append(grad_b)\n",
    "    # We built up lists of gradients backwards, so we reverse the lists\n",
    "    model['grad_weights'], model['grad_biases'] = grad_weights[::-1], grad_biases[::-1]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement a function to update the model parameters using computed gradients.\n",
    "\n",
    "Given some positive learning rate $\\eta>0$, we want to change all the weights and biases using their gradients.\n",
    "Write a function `update` to compute a single step of gradient descent assuming that the model gradients have been computed for a given input vector.\n",
    "+ The functions signature should be `update(eta, model)` where `eta` is a positive scalar value and `model` is a dictionary as output from `backward`.\n",
    "+ The result will be an updated model with the values updated for `model['weights']` and `model['biases']`.\n",
    "+ Written using array notations, these updates can be expressed as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   W^{\\ell} &\\leftarrow W^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{E} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   b^{\\ell} &\\leftarrow b^{\\ell} - \\eta \\nabla_{b^{\\ell}} \\mathcal{E} &&\n",
    "   \\end{aligned}.\n",
    "   $$\n",
    "+ Written out component-wise, the preceding array expressions would be written as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "      W^{\\ell}_{p,q} &\\leftarrow W^{\\ell}_{p,q} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}}\n",
    "      &&(\\ell=1,\\dotsc,L)\\\\\n",
    "      b^{\\ell}_{r} &\\leftarrow b^{\\ell}_{r} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &&\n",
    "   \\end{aligned}\n",
    "   $$.\n",
    "+ For safety, have the update step delete the keys added by calls to `forward` and `backward`, i.e., the keys `'z_inputs'`, `'activations'`, `'grad_weights'`, & `'grad_biases'`.\n",
    "+ The output should be a dict `model` like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters\n",
    "       eta : learning rate (positive scalar parameter)\n",
    "       model : dict with same keys as output by backward\n",
    "    Output result is a modified dict model\n",
    "    '''\n",
    "    new_weights, new_biases = [], []\n",
    "    for W, b, dW, db in zip(model['weights'], model['biases'], model['grad_weights'], model['grad_biases']):\n",
    "        new_weights.append(W - (eta * dW))\n",
    "        new_biases.append(b- (eta * db))\n",
    "    model['weights'] = new_weights\n",
    "    model['biases'] = new_biases\n",
    "    # Get rid of extraneous keys/values\n",
    "    for key in ['z_inputs', 'activations', 'grad_weights', 'grad_biases']:\n",
    "        del model[key]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement steepest descent in a loop for random training data\n",
    "\n",
    "Let's now attempt to use our NumPy-based model to implement the steepest descent algorithm. We'll explain these numbers shortly in the context of the MNIST digit classification problem.\n",
    "\n",
    "+ Generate random arrays `X` and `y` of dimensions $28^2 \\times N_{\\mathrm{batch}}$ and $10\\times N_{\\mathrm{batch}}$ respectively where $N_{\\mathrm{batch}}=10$.\n",
    "+ Initialize the network architecture using `initialize_model` as above to require an input layer of $28^2$ units, a hidden layer of 15 units, and an output layer of 10 units.\n",
    "+ Choose a learning rate of, say, $\\eta=0.5$ and a number of epochs `n_epoch` of, say, $30$.\n",
    "+ Construct a for loop with `n_epochs` iterations in which:\n",
    "    + The output `yhat` is computed from the input`X` using `forward`.\n",
    "    + The function `backward` is called to compute the gradients of the loss function with respect to the weights and biases.\n",
    "    + Update the network parameters using the function `update`.\n",
    "    + Compute and print out the epoch (iteration counter) and the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 56.93518157189513\n",
      "Epoch: 1\tLoss: 46.185688333319725\n",
      "Epoch: 2\tLoss: 44.30500741992865\n",
      "Epoch: 3\tLoss: 43.42197214793739\n",
      "Epoch: 4\tLoss: 42.74817041374757\n",
      "Epoch: 5\tLoss: 42.19285857512054\n",
      "Epoch: 6\tLoss: 41.754295971466185\n",
      "Epoch: 7\tLoss: 41.369371856888634\n",
      "Epoch: 8\tLoss: 40.96955225438356\n",
      "Epoch: 9\tLoss: 40.76088870589905\n",
      "Epoch: 10\tLoss: 40.59394755669327\n",
      "Epoch: 11\tLoss: 40.444111262232546\n",
      "Epoch: 12\tLoss: 40.30303698612012\n",
      "Epoch: 13\tLoss: 40.169348957516085\n",
      "Epoch: 14\tLoss: 40.028072982202225\n",
      "Epoch: 15\tLoss: 39.83248462582061\n",
      "Epoch: 16\tLoss: 39.31978555887973\n",
      "Epoch: 17\tLoss: 38.87527860779275\n",
      "Epoch: 18\tLoss: 38.53427953460131\n",
      "Epoch: 19\tLoss: 38.1706008762236\n",
      "Epoch: 20\tLoss: 37.898548341318914\n",
      "Epoch: 21\tLoss: 37.61807596673233\n",
      "Epoch: 22\tLoss: 37.31357136314796\n",
      "Epoch: 23\tLoss: 37.04579805770505\n",
      "Epoch: 24\tLoss: 36.74926618781042\n",
      "Epoch: 25\tLoss: 36.46855188390438\n",
      "Epoch: 26\tLoss: 36.279575494152795\n",
      "Epoch: 27\tLoss: 36.12783571947411\n",
      "Epoch: 28\tLoss: 35.99324279820415\n",
      "Epoch: 29\tLoss: 35.87061078665903\n"
     ]
    }
   ],
   "source": [
    "N_batch = 10\n",
    "n_epochs = 30\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1], N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    err = loss(yhat, y)\n",
    "    print(f'Epoch: {epoch}\\tLoss: {err}')\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modify the steepest descent loop to make a plot\n",
    "\n",
    "Let's alter the preceding loop to accumulate selected epoch & loss values in lists for plotting.\n",
    "\n",
    "+ Set `N_batch` and `n_epochs` to be larger, say, $50$ and $30,000$ respectively.\n",
    "+ Change the preceding `for` loop so that:\n",
    "    + The `epoch` counter and the loss value are accumulated into lists every, say, `SKIP` iterations where `SKIP==500`.\n",
    "    + Eliminate the `print` statement(s) to save on output.\n",
    "+ After the `for` loop terminates, make a `semilogy` plot to verify that the loss function is actually decreasing with sucessive epochs.\n",
    "    + Use the list `epochs` to accumulate the `epoch` every 500 epochs.\n",
    "    + Use the list `losses` to accumulate the values of the loss function every 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 50\n",
    "n_epochs = 30000\n",
    "SKIP = 50\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1], N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "# accumulate the epoch and loss in these respective lists\n",
    "epochs, losses = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "    if (divmod(epoch, SKIP)[1]==0):\n",
    "        err = loss(yhat, y)\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code for plotting once that the lists epochs and losses are accumulated\n",
    "fig = plt.figure(); ax = fig.add_subplot(111)\n",
    "ax.set_xlim([0,n_epochs]); ax.set_ylim([min(losses), max(losses)]);\n",
    "ax.set_xticks(epochs[::500]); ax.set_xlabel(\"Epochs\"); ax.grid(True);\n",
    "ax.set_ylabel(r'$\\mathcal{E}$'); \n",
    "h1 = ax.semilogy(epochs, losses, 'r-', label=r'$\\mathcal{E}$')\n",
    "plt.title('Loss vs. epochs');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
